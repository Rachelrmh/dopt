<!DOCTYPE html><html><head>
<meta charset="UTF-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Module dopt.nnet</title>
<link rel="stylesheet" href="../css/style.min.css"/>
<script type="text/javascript" src="../js/script.min.js"></script></head><body onload="setupDdox();"><header></header><nav id="main-nav"><div><noscript><p style="color: red">The search functionality needs JavaScript enabled</p></noscript><div id="symbolSearchPane" style="display: none"><form action="#" method="GET">
<input id="symbolSearch" type="text" name="q" placeholder="Search for symbols" autocomplete="off" onchange="performSymbolSearch(24);" onkeypress="this.onchange();" onpaste="this.onchange();" oninput="this.onchange();" autofocus/></form><ul id="symbolSearchResults" class="symbolList" style="display: none"></ul><script type="application/javascript" src="../symbols.js"></script><script type="application/javascript">var symbolSearchRootDir = "../";
document.getElementById('symbolSearchPane').style.display = 'block';</script></div><ul class="tree-view"><li class="tree-view "><div class="package ">dopt</div><ul class="tree-view"><li class="tree-view collapsed"><div class="package "><a href="../dopt/core.html">core</a></div><ul class="tree-view"><li class="tree-view collapsed"><div class="package "><a href="../dopt/core/ops.html">ops</a></div><ul class="tree-view"><li><div class="module "><a href="../dopt/core/ops/basic.html">basic</a></div></li><li><div class="module "><a href="../dopt/core/ops/math.html">math</a></div></li><li><div class="module "><a href="../dopt/core/ops/nnet.html">nnet</a></div></li><li><div class="module "><a href="../dopt/core/ops/random.html">random</a></div></li></ul></li><li><div class="module "><a href="../dopt/core/cpu.html">cpu</a></div></li><li><div class="module "><a href="../dopt/core/cuda.html">cuda</a></div></li><li><div class="module "><a href="../dopt/core/grads.html">grads</a></div></li></ul></li><li class="tree-view "><div class="package selected"><a href="../dopt/nnet.html">nnet</a></div><ul class="tree-view"><li class="tree-view collapsed"><div class="package "><a href="../dopt/nnet/layers.html">layers</a></div><ul class="tree-view"><li><div class="module "><a href="../dopt/nnet/layers/batchnorm.html">batchnorm</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/conv.html">conv</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/datasource.html">datasource</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/dense.html">dense</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/dropout.html">dropout</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/maxpool.html">maxpool</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/relu.html">relu</a></div></li><li><div class="module "><a href="../dopt/nnet/layers/softmax.html">softmax</a></div></li></ul></li><li><div class="module "><a href="../dopt/nnet/losses.html">losses</a></div></li><li><div class="module "><a href="../dopt/nnet/networks.html">networks</a></div></li><li><div class="module "><a href="../dopt/nnet/parameters.html">parameters</a></div></li></ul></li><li class="tree-view collapsed"><div class="package "><a href="../dopt/online.html">online</a></div><ul class="tree-view"><li><div class="module "><a href="../dopt/online/adam.html">adam</a></div></li><li><div class="module "><a href="../dopt/online/amsgrad.html">amsgrad</a></div></li><li><div class="module "><a href="../dopt/online/sgd.html">sgd</a></div></li></ul></li></ul></li></ul></div><p id="main-nav-footer">Built with
<a href="https://github.com/MartinNowak/scod">scod</a></p></nav><div id="main-contents"><div><h1>Module dopt.nnet</h1><p>This package contains a deep learning API backed by dopt.
</p><section><p>Working examples for how this package can be used are given in the <code class="lang-d"><span class="pln">examples</span><span class="pun">/</span><span class="pln">mnist<wbr/></span><span class="pun">.</span><span class="pln">d</span></code> and <code class="lang-d"><span class="pln">examples</span><span class="pun">/</span><span class="pln">cifar10<wbr/></span><span class="pun">.</span><span class="pln">d</span></code>
    files.
</p>

<p>    One would generally start by using UFCS to define a feed-forward network:
</p>

<pre class="code"><code class="lang-d"><span class="kwd">auto </span><span class="pln">features </span><span class="pun">= </span><span class="pln">float32</span><span class="pun">([</span><span class="lit">128</span><span class="pun">, </span><span class="lit">1</span><span class="pun">, </span><span class="lit">28</span><span class="pun">, </span><span class="lit">28</span><span class="pun">]);

</span><span class="kwd">auto </span><span class="pln">layers </span><span class="pun">= </span><span class="pln">dataSource</span><span class="pun">(</span><span class="pln">features</span><span class="pun">)
             <wbr/>.</span><span class="pln">dense</span><span class="pun">(</span><span class="lit">2_000</span><span class="pun">)
             <wbr/>.</span><span class="pln">relu</span><span class="pun">()
             <wbr/>.</span><span class="pln">dense</span><span class="pun">(</span><span class="lit">2_000</span><span class="pun">)
             <wbr/>.</span><span class="pln">relu</span><span class="pun">()
             <wbr/>.</span><span class="pln">dense</span><span class="pun">(</span><span class="lit">10</span><span class="pun">)
             <wbr/>.</span><span class="pln">softmax</span><span class="pun">();</span></code></pre>

<p>    The <code class="lang-d"><span class="typ">DAGNetwork</span></code> class can then be used to traverse the resulting graph and aggregate parameters/loss terms:
</p>

<pre class="code"><code class="lang-d"><span class="kwd">auto </span><span class="pln">network </span><span class="pun">= </span><span class="kwd">new </span><span class="typ">DAGNetwork</span><span class="pun">([</span><span class="pln">features</span><span class="pun">], </span><span class="pln">layers</span><span class="pun">);</span></code></pre>

<p>    After this, one can define an objective function---there are a few standard loss functions implemented in
    <code class="lang-d"><a href="../dopt/nnet/losses.html"><span class="pln">dopt<wbr/></span><span class="pun">.</span><span class="pln">nnet<wbr/></span><span class="pun">.</span><span class="pln">losses</span></a></code>:
</p>

<pre class="code"><code class="lang-d"><span class="kwd">auto </span><span class="pln">labels </span><span class="pun">= </span><span class="pln">float32</span><span class="pun">([</span><span class="lit">128</span><span class="pun">, </span><span class="lit">10</span><span class="pun">]);

</span><span class="kwd">auto </span><span class="pln">trainLoss </span><span class="pun">= </span><span class="pln">crossEntropy</span><span class="pun">(</span><span class="pln">layers<wbr/></span><span class="pun">.</span><span class="pln">trainOutput</span><span class="pun">, </span><span class="pln">labels</span><span class="pun">) + </span><span class="pln">network<wbr/></span><span class="pun">.</span><span class="pln">paramLoss</span><span class="pun">;</span></code></pre>

<p>    where <code class="lang-d"><span class="pln">network<wbr/></span><span class="pun">.</span><span class="pln">paramLoss</span></code> is the sum of any parameter regularisation terms. The <code class="lang-d"><a href="../dopt/online.html"><span class="pln">dopt<wbr/></span><span class="pun">.</span><span class="pln">online</span></a></code> package can be
    used to construct an updater:
</p>

<pre class="code"><code class="lang-d"><span class="kwd">auto </span><span class="pln">updater </span><span class="pun">= </span><span class="pln">sgd</span><span class="pun">([</span><span class="pln">trainLoss</span><span class="pun">], </span><span class="pln">network<wbr/></span><span class="pun">.</span><span class="pln">params</span><span class="pun">, </span><span class="pln">network<wbr/></span><span class="pun">.</span><span class="pln">paramProj</span><span class="pun">);</span></code></pre>

<p>    Finally, one can call this updater with some actual training data:
</p>

<pre class="code"><code class="lang-d"><span class="pln">updater</span><span class="pun">([
    </span><span class="pln">features</span><span class="pun">: </span><span class="typ">Buffer</span><span class="pun">(</span><span class="pln">some_real_features</span><span class="pun">),
    </span><span class="pln">labels</span><span class="pun">: </span><span class="typ">Buffer</span><span class="pun">(</span><span class="pln">some_real_labels</span><span class="pun">)
]);</span></code></pre>
</section>

<section></section></div><footer><div id="license-info"><p>Henry Gouk
</p>


</div></footer></div></body></html>